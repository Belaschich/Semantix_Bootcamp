# Semantix_Bootcamp
 Treinamento de Big Data Engineer no Semantix Academy

   **Trilha de Estudo:**

   **Big Data Foundations (Semana 1, 2 e 3)** 
  
 - [X] Conhecimento de ferramentas atuais no mercado de Big Data;
 - [X] Criação e funcionamento de um cluster Hadoop para Big Data em Docker;
 - [X] Manipulação de dados com HDFS;
 - [X] Manipulação de dados com uso do Hive;
 - [X] Otimização de consultas em grandes volumes de dados estruturados e semiestruturados com uso de Hive;
 - [X] Ingestão de dados relacionais para o HDFS/Hive, com uso do Sqoop;
 - [X] Otimização de importação no Sqoop;
 - [X] Exportação de dados do HDFS para o SGBD, com uso do Sqoop;
 - [X] Manipulação de dados com HBase;
 - [X] Operações com Dataframe em Spark para processamento de dados em batch;
 - [X] Uso do Spark SQL Queries para consultas de dados estruturados e semiestruturados.

   **MongoDB - Básico (Semana 4)**

 - [X] Entendimento de conceitos e arquitetura NoSQL e MongoDB;
 - [X] Instalação de cluster MongoDB através de container e Cloud;
 - [X] Manipular coleções, documentos e índices;
 - [X] Realizar diversas pesquisas no MongoDB com diferentes operadores;
 - [X] Fazer uso das interfaces gráficas MongoExpress e MongoCompass;
 - [X] Trabalhar com pipeline de agregações;
 - [X] Entendimento de Replicação e shards.
 
   **Apache Kafka – Básico (Semana 5)**

 - [X] Entendimento de conceitos e arquitetura do Kafka e da Confluent;
 - [X] Instalação de cluster Kafka através de container;
 - [X] Gerenciamento de tópicos;
 - [X] Produção e consumo de dados através do console;
 - [X] Entendimento das guias do Control Center;
 - [X] Desenvolvimento de stream com uso do KSQL;
 - [X] Aplicação de KSQL Datagen;
 - [X] Produção e consumo de dados com uso do Schema Registry;
 - [X] Trabalhando com Kafka Connect;
 - [X] Custos com Confluent Cloud;
 - [X] Otimização de parâmetros;
 - [X] Melhores práticas em um cluster Kafka.

   **Redis – Básico (Semana 6)**

 - [X] Entendimento de conceitos e arquitetura NoSQL e Redis;
 - [X] Instalação de cluster Redis através de container;
 - [X] Manipulação de diversos tipos de estrutura de dados com Redis-CLI;
 - [X] Implementar paradigma de mensagens Pub/Sub;
 - [X] Configurações básicas de persistência de dados.
 
   **Elastic Essential I (Semana 7 e 8):**

 - [X] Entendimento de conceitos e arquitetura da Elastic;
 - [X] Instalação de cluster Elastic através de container;
 - [X] Realizar operações de CRUD em índices;
 - [X] Gerenciamento de índices;
 - [X] Alteração de mapeamento e reindex;
 - [X] Desenvolvimento de consultas do tipo term, terms, range, match e multi_match, com uso de bool query;
 - [X] Aplicação de analyzers em atributos;
 - [X] Desenvolvimento de agregações básicas;
 - [X] Ingestão de dados através de beats e logstash;
 - [X] Entendimento das guias do Kibana;

   **Spark - Big Data Processing (Semana 9, 10 e 11)**

 - [ ] Uso do Jupyter Notebooks para a criação de projetos em Spark com Python
 - [ ] Spark batch intermediario
 - [ ] Operações com RDD em Spark para processamento de dados em batch;
 - [ ] Uso de Partições com RDD;
 - [ ] Operações com Dataset em Spark para processamento de dados em batch;
 - [ ] Uso de Dataset em Dataframe e RDD;
 - [ ] Comandos avançados com Dataset;
 - [ ] Uso do IntelliJ IDEA para a criação de projetos em Spark com Scala;
 - [ ] Struct Streaming para leitura de dados do Kafka;
 - [ ] Spark Streaming para leitura de dados do Kafka;
 - [ ] Otimizações com uso de Variáveis Compartilhadas;
 - [ ] Criações de User defined Function;
 - [ ] Configurações de Tunning para o Spark Application.

  **Computador necessário para o treinamento:**

    Sistema Operacional de 64 bits;

    Memória RAM de 8 GB;

    Acesso a internet;

    HD com no mínimo 50 GB de espaço livre.

**Computador utilizado para o treinamento:**

    **Montei uma máquina virtual no GCP**

    Sistema Operacional de 64 bits; (Linux - Ubuntu) 

    Memória RAM de 16 GB; 

    Acesso a internet;

    HD com no mínimo 50 GB de espaço livre.